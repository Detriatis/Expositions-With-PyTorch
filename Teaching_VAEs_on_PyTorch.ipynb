{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOZBSRbG_ziQ"
      },
      "source": [
        "#Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYxvhB-1ZAFz"
      },
      "source": [
        "References for Details on Conditional Distributions & KL-Divergence (might be helpful)\n",
        "\n",
        "    https://www.probabilitycourse.com/chapter1/1_4_0_conditional_probability.php\n",
        "\n",
        "    https://home.ttic.edu/~madhurt/courses/infotheory2021/\n",
        "\n",
        "\n",
        "Miscellaneous References (might be helpful)\n",
        "\n",
        "    https://github.com/Jackson-Kang/Pytorch-VAE-tutorial/blob/master/01_Variational_AutoEncoder.ipynb\n",
        "\n",
        "    https://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UtpC-4IdAhY3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "stjxdc7YAhY3",
        "outputId": "736c8328-3303-4c37-adaf-5b58b19ef588"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7c922b4d5930>]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1JklEQVR4nO3df1yV9f3/8efB+GXCUVR+JQamoc2fmSHa/HGTiWYWm9tH/djUxqw1aBKuGlta9lkf/DHdbm1O50zNNWPz80lcztkIE3MgiOn8YkbZRNIAy4KjmIjw/v7hzfPZCUwP/kDePe6323WL875e7/d5v73Q8+ziui4cxhgjAACANs6ntScAAABwNRBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWuKm1J3C9NDY26qOPPlJQUJAcDkdrTwcAAFwGY4xOnjypyMhI+fh8+bmYr0yo+eijjxQVFdXa0wAAAC3w4Ycfqlu3bl9a85UJNUFBQZLO/6EEBwe38mwAAMDlcLlcioqKcn+Of5mvTKi58COn4OBgQg0AAG3M5Vw6woXCAADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKXoWazMxMDRkyREFBQQoNDVVSUpJKS0u/tM+BAwc0adIkRUdHy+Fw6Fe/+lWzdcuWLVN0dLQCAgIUFxenoqIij/1nzpxRSkqKOnfurA4dOmjSpEmqqqryZvoAAMBiXoWavLw8paSkaNeuXcrJyVF9fb3Gjh2r2trai/Y5ffq0evTooQULFig8PLzZmj/96U9KT0/XM888o7ffflsDBgxQYmKijh8/7q55/PHH9dprr2nDhg3Ky8vTRx99pG9961veTB8AAFjMYYwxLe388ccfKzQ0VHl5eRoxYsQl66Ojo5WWlqa0tDSP9ri4OA0ZMkS/+c1vJEmNjY2KiorSY489pp/85CeqqalR165dtX79en3729+WJL377rvq06ePCgoKNHTo0Eu+t8vlktPpVE1NDb/7CQCANsKbz+8ruqampqZGkhQSEtLiMc6ePas9e/YoISHh/ybl46OEhAQVFBRIkvbs2aP6+nqPmt69e6t79+7umi+qq6uTy+Xy2AAAgL1aHGoaGxuVlpam4cOHq2/fvi2ewCeffKKGhgaFhYV5tIeFhamyslKSVFlZKT8/P3Xs2PGiNV+UmZkpp9Pp3qKiolo8RwAAcONrcahJSUlRSUmJsrKyruZ8rpqMjAzV1NS4tw8//LC1pwQAAK6hm1rSKTU1VZs3b9aOHTvUrVu3K5pAly5d1K5duyZ3MlVVVbkvLA4PD9fZs2dVXV3tcbbm32u+yN/fX/7+/lc0NwAA0HZ4dabGGKPU1FRt3LhR27ZtU0xMzBVPwM/PT4MHD1Zubq67rbGxUbm5uYqPj5ckDR48WL6+vh41paWlKi8vd9cAAICvNq/O1KSkpGj9+vXatGmTgoKC3NezOJ1OBQYGSpKmT5+uW265RZmZmZLOXwj8zjvvuL8+duyY9u3bpw4dOqhnz56SpPT0dM2YMUN33XWX7r77bv3qV79SbW2tHnroIff4ycnJSk9PV0hIiIKDg/XYY48pPj7+su58AgAA9vPqlm6Hw9Fs+5o1azRz5kxJ0qhRoxQdHa21a9dKksrKypo9ozNy5Eht377d/fo3v/mNFi9erMrKSg0cOFAvvPCC4uLi3PvPnDmjOXPm6JVXXlFdXZ0SExP129/+9qI/fvoibukGAKDt8ebz+4qeU9OWEGoAAGh7rttzagAAAG4UhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACt4FWoyMzM1ZMgQBQUFKTQ0VElJSSotLb1kvw0bNqh3794KCAhQv379tGXLFo/9Doej2W3x4sXumujo6Cb7FyxY4M30AQCAxbwKNXl5eUpJSdGuXbuUk5Oj+vp6jR07VrW1tRftk5+fr6lTpyo5OVl79+5VUlKSkpKSVFJS4q6pqKjw2FavXi2Hw6FJkyZ5jPXcc8951D322GNeLhcAANjKYYwxLe388ccfKzQ0VHl5eRoxYkSzNZMnT1Ztba02b97sbhs6dKgGDhyoFStWNNsnKSlJJ0+eVG5urrstOjpaaWlpSktLa9FcXS6XnE6nampqFBwc3KIxAADA9eXN5/cVXVNTU1MjSQoJCbloTUFBgRISEjzaEhMTVVBQ0Gx9VVWV/vrXvyo5ObnJvgULFqhz584aNGiQFi9erHPnzl30fevq6uRyuTw2AABgr5ta2rGxsVFpaWkaPny4+vbte9G6yspKhYWFebSFhYWpsrKy2fqXXnpJQUFB+ta3vuXR/qMf/Uh33nmnQkJClJ+fr4yMDFVUVGjp0qXNjpOZman58+d7uSoAANBWtTjUpKSkqKSkRDt37rya89Hq1as1bdo0BQQEeLSnp6e7v+7fv7/8/Pz0yCOPKDMzU/7+/k3GycjI8OjjcrkUFRV1VecKAABuHC0KNampqdq8ebN27Nihbt26fWlteHi4qqqqPNqqqqoUHh7epPatt95SaWmp/vSnP11yDnFxcTp37pzKysoUGxvbZL+/v3+zYQcAANjJq2tqjDFKTU3Vxo0btW3bNsXExFyyT3x8vMcFv5KUk5Oj+Pj4JrUvvviiBg8erAEDBlxy3H379snHx0ehoaGXvwAAAGAtr87UpKSkaP369dq0aZOCgoLc18U4nU4FBgZKkqZPn65bbrlFmZmZkqTZs2dr5MiRWrJkiSZMmKCsrCwVFxdr5cqVHmO7XC5t2LBBS5YsafK+BQUFKiws1OjRoxUUFKSCggI9/vjjevDBB9WpU6cWLRwAANjFq1CzfPlySdKoUaM82tesWaOZM2dKksrLy+Xj838ngIYNG6b169fr6aef1k9/+lP16tVL2dnZTS4uzsrKkjFGU6dObfK+/v7+ysrK0rPPPqu6ujrFxMTo8ccf97hmBgAAfLVd0XNq2hKeUwMAQNtz3Z5TAwAAcKMg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWMGrUJOZmakhQ4YoKChIoaGhSkpKUmlp6SX7bdiwQb1791ZAQID69eunLVu2eOyfOXOmHA6HxzZu3DiPmk8//VTTpk1TcHCwOnbsqOTkZJ06dcqb6QMAAIt5FWry8vKUkpKiXbt2KScnR/X19Ro7dqxqa2sv2ic/P19Tp05VcnKy9u7dq6SkJCUlJamkpMSjbty4caqoqHBvr7zyisf+adOm6cCBA8rJydHmzZu1Y8cOPfzww95MHwAAWMxhjDEt7fzxxx8rNDRUeXl5GjFiRLM1kydPVm1trTZv3uxuGzp0qAYOHKgVK1ZIOn+mprq6WtnZ2c2OcfDgQd1xxx3avXu37rrrLknS1q1bde+99+ro0aOKjIy85FxdLpecTqdqamoUHBzs5UoBAEBr8Obz+4quqampqZEkhYSEXLSmoKBACQkJHm2JiYkqKCjwaNu+fbtCQ0MVGxurRx99VCdOnPAYo2PHju5AI0kJCQny8fFRYWFhs+9bV1cnl8vlsQEAAHu1ONQ0NjYqLS1Nw4cPV9++fS9aV1lZqbCwMI+2sLAwVVZWul+PGzdO69atU25urhYuXKi8vDyNHz9eDQ0N7jFCQ0M9xrjpppsUEhLiMc6/y8zMlNPpdG9RUVEtXSoAAGgDbmppx5SUFJWUlGjnzp1XPIkpU6a4v+7Xr5/69++v2267Tdu3b9eYMWNaNGZGRobS09Pdr10uF8EGAACLtehMTWpqqjZv3qw333xT3bp1+9La8PBwVVVVebRVVVUpPDz8on169OihLl266NChQ+4xjh8/7lFz7tw5ffrppxcdx9/fX8HBwR4bAACwl1ehxhij1NRUbdy4Udu2bVNMTMwl+8THxys3N9ejLScnR/Hx8Rftc/ToUZ04cUIRERHuMaqrq7Vnzx53zbZt29TY2Ki4uDhvlgAAACzlVahJSUnRyy+/rPXr1ysoKEiVlZWqrKzU559/7q6ZPn26MjIy3K9nz56trVu3asmSJXr33Xf17LPPqri4WKmpqZKkU6dO6YknntCuXbtUVlam3NxcPfDAA+rZs6cSExMlSX369NG4ceM0a9YsFRUV6R//+IdSU1M1ZcqUy7rzCQAA2M+rULN8+XLV1NRo1KhRioiIcG9/+tOf3DXl5eWqqKhwvx42bJjWr1+vlStXasCAAfqf//kfZWdnuy8ubteunfbv36/7779ft99+u5KTkzV48GC99dZb8vf3d4/zxz/+Ub1799aYMWN077336p577tHKlSuvdP0AAMASV/ScmraE59QAAND2XLfn1AAAANwoCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFbwKtRkZmZqyJAhCgoKUmhoqJKSklRaWnrJfhs2bFDv3r0VEBCgfv36acuWLe599fX1euqpp9SvXz/dfPPNioyM1PTp0/XRRx95jBEdHS2Hw+GxLViwwJvpAwAAi3kVavLy8pSSkqJdu3YpJydH9fX1Gjt2rGpray/aJz8/X1OnTlVycrL27t2rpKQkJSUlqaSkRJJ0+vRpvf3225o7d67efvttvfrqqyotLdX999/fZKznnntOFRUV7u2xxx7zcrkAAMBWDmOMaWnnjz/+WKGhocrLy9OIESOarZk8ebJqa2u1efNmd9vQoUM1cOBArVixotk+u3fv1t13360jR46oe/fuks6fqUlLS1NaWlqL5upyueR0OlVTU6Pg4OAWjQEAAK4vbz6/r+iampqaGklSSEjIRWsKCgqUkJDg0ZaYmKiCgoIvHdfhcKhjx44e7QsWLFDnzp01aNAgLV68WOfOnbvoGHV1dXK5XB4bAACw100t7djY2Ki0tDQNHz5cffv2vWhdZWWlwsLCPNrCwsJUWVnZbP2ZM2f01FNPaerUqR6J7Ec/+pHuvPNOhYSEKD8/XxkZGaqoqNDSpUubHSczM1Pz589vwcoAAEBb1OJQk5KSopKSEu3cufOqTaa+vl7/8R//IWOMli9f7rEvPT3d/XX//v3l5+enRx55RJmZmfL3928yVkZGhkcfl8ulqKioqzZXAABwY2lRqElNTdXmzZu1Y8cOdevW7Utrw8PDVVVV5dFWVVWl8PBwj7YLgebIkSPatm3bJX9uFhcXp3PnzqmsrEyxsbFN9vv7+zcbdgAAgJ28uqbGGKPU1FRt3LhR27ZtU0xMzCX7xMfHKzc316MtJydH8fHx7tcXAs3777+vN954Q507d77kuPv27ZOPj49CQ0O9WQIAALCUV2dqUlJStH79em3atElBQUHu62KcTqcCAwMlSdOnT9ctt9yizMxMSdLs2bM1cuRILVmyRBMmTFBWVpaKi4u1cuVKSecDzbe//W29/fbb2rx5sxoaGtzjhoSEyM/PTwUFBSosLNTo0aMVFBSkgoICPf7443rwwQfVqVOnq/aHAQAA2i6vbul2OBzNtq9Zs0YzZ86UJI0aNUrR0dFau3ate/+GDRv09NNPq6ysTL169dKiRYt07733SpLKysouesbnzTff1KhRo/T222/rhz/8od59913V1dUpJiZG3/3ud5Wenn7ZP2Lilm4AANoebz6/r+g5NW0JoQYAgLbnuj2nBgAA4EZBqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsIJXoSYzM1NDhgxRUFCQQkNDlZSUpNLS0kv227Bhg3r37q2AgAD169dPW7Zs8dhvjNG8efMUERGhwMBAJSQk6P333/eo+fTTTzVt2jQFBwerY8eOSk5O1qlTp7yZPgAAsJhXoSYvL08pKSnatWuXcnJyVF9fr7Fjx6q2tvaiffLz8zV16lQlJydr7969SkpKUlJSkkpKStw1ixYt0gsvvKAVK1aosLBQN998sxITE3XmzBl3zbRp03TgwAHl5ORo8+bN2rFjhx5++OEWLBkAANjIYYwxLe388ccfKzQ0VHl5eRoxYkSzNZMnT1Ztba02b97sbhs6dKgGDhyoFStWyBijyMhIzZkzRz/+8Y8lSTU1NQoLC9PatWs1ZcoUHTx4UHfccYd2796tu+66S5K0detW3XvvvTp69KgiIyMvOVeXyyWn06mamhoFBwe3dMlNGGP0eX3DVRsPAIC2LNC3nRwOx1Ubz5vP75uu5I1qamokSSEhIRetKSgoUHp6ukdbYmKisrOzJUmHDx9WZWWlEhIS3PudTqfi4uJUUFCgKVOmqKCgQB07dnQHGklKSEiQj4+PCgsL9c1vfrPJ+9bV1amurs792uVytWiNl/J5fYPumPf6NRkbAIC25p3nEtXe74riRYu1+ELhxsZGpaWlafjw4erbt+9F6yorKxUWFubRFhYWpsrKSvf+C21fVhMaGuqx/6abblJISIi75osyMzPldDrdW1RUlHcLBAAAbUqLo1RKSopKSkq0c+fOqzmfqyYjI8PjDJHL5bomwSbQt53eeS7xqo8LAEBbFOjbrtXeu0WhJjU11X2xbrdu3b60Njw8XFVVVR5tVVVVCg8Pd++/0BYREeFRM3DgQHfN8ePHPcY4d+6cPv30U3f/L/L395e/v79X62oJh8PRaqfZAADA//Hqx0/GGKWmpmrjxo3atm2bYmJiLtknPj5eubm5Hm05OTmKj4+XJMXExCg8PNyjxuVyqbCw0F0THx+v6upq7dmzx12zbds2NTY2Ki4uzpslAAAAS3l1iiElJUXr16/Xpk2bFBQU5L6exel0KjAwUJI0ffp03XLLLcrMzJQkzZ49WyNHjtSSJUs0YcIEZWVlqbi4WCtXrpR0/kxHWlqafv7zn6tXr16KiYnR3LlzFRkZqaSkJElSnz59NG7cOM2aNUsrVqxQfX29UlNTNWXKlMu68wkAAHwFGC9IanZbs2aNu2bkyJFmxowZHv3+/Oc/m9tvv934+fmZr33ta+avf/2rx/7GxkYzd+5cExYWZvz9/c2YMWNMaWmpR82JEyfM1KlTTYcOHUxwcLB56KGHzMmTJy977jU1NUaSqamp8WbJAACgFXnz+X1Fz6lpS67Vc2oAAMC1483nN7/7CQAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBa9DzY4dOzRx4kRFRkbK4XAoOzv7kn2WLVumPn36KDAwULGxsVq3bp3H/lGjRsnhcDTZJkyY4K6ZOXNmk/3jxo3zdvoAAMBSN3nboba2VgMGDND3vvc9fetb37pk/fLly5WRkaHf//73GjJkiIqKijRr1ix16tRJEydOlCS9+uqrOnv2rLvPiRMnNGDAAH3nO9/xGGvcuHFas2aN+7W/v7+30wcAAJbyOtSMHz9e48ePv+z6P/zhD3rkkUc0efJkSVKPHj20e/duLVy40B1qQkJCPPpkZWWpffv2TUKNv7+/wsPDvZ0yAAD4Crjm19TU1dUpICDAoy0wMFBFRUWqr69vts+LL76oKVOm6Oabb/Zo3759u0JDQxUbG6tHH31UJ06c+NL3dblcHhsAALDXNQ81iYmJWrVqlfbs2SNjjIqLi7Vq1SrV19frk08+aVJfVFSkkpISff/73/doHzdunNatW6fc3FwtXLhQeXl5Gj9+vBoaGpp938zMTDmdTvcWFRV1TdYHAABuDA5jjGlxZ4dDGzduVFJS0kVrPv/8c6WkpOgPf/iDjDEKCwvTgw8+qEWLFqmyslJhYWEe9Y888ogKCgq0f//+L33vf/3rX7rtttv0xhtvaMyYMU3219XVqa6uzv3a5XIpKipKNTU1Cg4O9m6hAACgVbhcLjmdzsv6/L7mZ2oCAwO1evVqnT59WmVlZSovL1d0dLSCgoLUtWtXj9ra2lplZWUpOTn5kuP26NFDXbp00aFDh5rd7+/vr+DgYI8NAADYy+sLhVvK19dX3bp1k3T+QuD77rtPPj6emWrDhg2qq6vTgw8+eMnxjh49qhMnTigiIuKazBcAALQtXoeaU6dOeZwdOXz4sPbt26eQkBB1795dGRkZOnbsmPtZNO+9956KiooUFxenzz77TEuXLlVJSYleeumlJmO/+OKLSkpKUufOnZu85/z58zVp0iSFh4frgw8+0JNPPqmePXsqMTHR2yUAAAALeR1qiouLNXr0aPfr9PR0SdKMGTO0du1aVVRUqLy83L2/oaFBS5YsUWlpqXx9fTV69Gjl5+crOjraY9zS0lLt3LlTf//735u8Z7t27bR//3699NJLqq6uVmRkpMaOHav/+q//4lk1AABA0hVeKNyWeHOhEQAAuDHcUBcKAwAAXA+EGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAK3gdanbs2KGJEycqMjJSDodD2dnZl+yzbNky9enTR4GBgYqNjdW6des89q9du1YOh8NjCwgI8KgxxmjevHmKiIhQYGCgEhIS9P7773s7fQAAYCmvQ01tba0GDBigZcuWXVb98uXLlZGRoWeffVYHDhzQ/PnzlZKSotdee82jLjg4WBUVFe7tyJEjHvsXLVqkF154QStWrFBhYaFuvvlmJSYm6syZM94uAQAAWOgmbzuMHz9e48ePv+z6P/zhD3rkkUc0efJkSVKPHj20e/duLVy4UBMnTnTXORwOhYeHNzuGMUa/+tWv9PTTT+uBBx6QJK1bt05hYWHKzs7WlClTvF0GAACwzDW/pqaurq7Jj5ICAwNVVFSk+vp6d9upU6d06623KioqSg888IAOHDjg3nf48GFVVlYqISHB3eZ0OhUXF6eCgoKLvq/L5fLYAACAva55qElMTNSqVau0Z88eGWNUXFysVatWqb6+Xp988okkKTY2VqtXr9amTZv08ssvq7GxUcOGDdPRo0clSZWVlZKksLAwj7HDwsLc+74oMzNTTqfTvUVFRV3DVQIAgNZ2zUPN3LlzNX78eA0dOlS+vr564IEHNGPGjPNv7nP+7ePj4zV9+nQNHDhQI0eO1KuvvqquXbvqd7/7XYvfNyMjQzU1Ne7tww8/vCrrAQAAN6ZrHmoCAwO1evVqnT59WmVlZSovL1d0dLSCgoLUtWvXZvv4+vpq0KBBOnTokCS5r7WpqqryqKuqqrrodTj+/v4KDg722AAAgL2u23NqfH191a1bN7Vr105ZWVm677773GdqvqihoUH/7//9P0VEREiSYmJiFB4ertzcXHeNy+VSYWGh4uPjr8v8AQDAjc3ru59OnTrlPoMinb+Id9++fQoJCVH37t2VkZGhY8eOuZ9F895776moqEhxcXH67LPPtHTpUpWUlOill15yj/Hcc89p6NCh6tmzp6qrq7V48WIdOXJE3//+9yWdvzMqLS1NP//5z9WrVy/FxMRo7ty5ioyMVFJS0hX+EQAAABt4HWqKi4s1evRo9+v09HRJ0owZM7R27VpVVFSovLzcvb+hoUFLlixRaWmpfH19NXr0aOXn5ys6Otpd89lnn2nWrFmqrKxUp06dNHjwYOXn5+uOO+5w1zz55JOqra3Vww8/rOrqat1zzz3aunVrkzurAADAV5PDGGNaexLXg8vlktPpVE1NDdfXAADQRnjz+c3vfgIAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWMHrULNjxw5NnDhRkZGRcjgcys7OvmSfZcuWqU+fPgoMDFRsbKzWrVvnsf/3v/+9vv71r6tTp07q1KmTEhISVFRU5FEzc+ZMORwOj23cuHHeTh8AAFjK61BTW1urAQMGaNmyZZdVv3z5cmVkZOjZZ5/VgQMHNH/+fKWkpOi1115z12zfvl1Tp07Vm2++qYKCAkVFRWns2LE6duyYx1jjxo1TRUWFe3vllVe8nT4AALCUwxhjWtzZ4dDGjRuVlJR00Zphw4Zp+PDhWrx4sbttzpw5Kiws1M6dO5vt09DQoE6dOuk3v/mNpk+fLun8mZrq6urLOjPUHJfLJafTqZqaGgUHB7doDAAAcH158/l9za+pqaurU0BAgEdbYGCgioqKVF9f32yf06dPq76+XiEhIR7t27dvV2hoqGJjY/Xoo4/qxIkTX/q+LpfLYwMAAPa65qEmMTFRq1at0p49e2SMUXFxsVatWqX6+np98sknzfZ56qmnFBkZqYSEBHfbuHHjtG7dOuXm5mrhwoXKy8vT+PHj1dDQ0OwYmZmZcjqd7i0qKuqarA8AANwYbrrWbzB37lxVVlZq6NChMsYoLCxMM2bM0KJFi+Tj0zRTLViwQFlZWdq+fbvHGZ4pU6a4v+7Xr5/69++v2267Tdu3b9eYMWOajJORkaH09HT3a5fLRbABAMBi1/xMTWBgoFavXq3Tp0+rrKxM5eXlio6OVlBQkLp27epR+4tf/EILFizQ3//+d/Xv3/9Lx+3Ro4e6dOmiQ4cONbvf399fwcHBHhsAALDXNT9Tc4Gvr6+6desmScrKytJ9993ncaZm0aJFev755/X666/rrrvuuuR4R48e1YkTJxQREXHN5gwAANoOr0PNqVOnPM6OHD58WPv27VNISIi6d++ujIwMHTt2zP0smvfee09FRUWKi4vTZ599pqVLl6qkpEQvvfSSe4yFCxdq3rx5Wr9+vaKjo1VZWSlJ6tChgzp06KBTp05p/vz5mjRpksLDw/XBBx/oySefVM+ePZWYmHilfwYAAMACXv/4qbi4WIMGDdKgQYMkSenp6Ro0aJDmzZsnSaqoqFB5ebm7vqGhQUuWLNGAAQP0jW98Q2fOnFF+fr6io6PdNcuXL9fZs2f17W9/WxEREe7tF7/4hSSpXbt22r9/v+6//37dfvvtSk5O1uDBg/XWW2/J39//StYPAAAscUXPqWlLeE4NAABtzw31nBoAAIDrgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFr0PNjh07NHHiREVGRsrhcCg7O/uSfZYtW6Y+ffooMDBQsbGxWrduXZOaDRs2qHfv3goICFC/fv20ZcsWj/3GGM2bN08REREKDAxUQkKC3n//fW+nDwAALOV1qKmtrdWAAQO0bNmyy6pfvny5MjIy9Oyzz+rAgQOaP3++UlJS9Nprr7lr8vPzNXXqVCUnJ2vv3r1KSkpSUlKSSkpK3DWLFi3SCy+8oBUrVqiwsFA333yzEhMTdebMGW+XAAAALOQwxpgWd3Y4tHHjRiUlJV20ZtiwYRo+fLgWL17sbpszZ44KCwu1c+dOSdLkyZNVW1urzZs3u2uGDh2qgQMHasWKFTLGKDIyUnPmzNGPf/xjSVJNTY3CwsK0du1aTZky5ZJzdblccjqdqqmpUXBwcAtXDAAAridvPr+v+TU1dXV1CggI8GgLDAxUUVGR6uvrJUkFBQVKSEjwqElMTFRBQYEk6fDhw6qsrPSocTqdiouLc9c0974ul8tjAwAA9rrmoSYxMVGrVq3Snj17ZIxRcXGxVq1apfr6en3yySeSpMrKSoWFhXn0CwsLU2VlpXv/hbaL1XxRZmamnE6ne4uKirraSwMAADeQax5q5s6dq/Hjx2vo0KHy9fXVAw88oBkzZpx/c59r9/YZGRmqqalxbx9++OE1ey8AAND6rnmoCQwM1OrVq3X69GmVlZWpvLxc0dHRCgoKUteuXSVJ4eHhqqqq8uhXVVWl8PBw9/4LbRer+SJ/f38FBwd7bAAAwF7X7Tk1vr6+6tatm9q1a6esrCzdd9997jM18fHxys3N9ajPyclRfHy8JCkmJkbh4eEeNS6XS4WFhe4aAADw1XaTtx1OnTqlQ4cOuV8fPnxY+/btU0hIiLp3766MjAwdO3bM/Sya9957T0VFRYqLi9Nnn32mpUuXqqSkRC+99JJ7jNmzZ2vkyJFasmSJJkyYoKysLBUXF2vlypWSzt9llZaWpp///Ofq1auXYmJiNHfuXEVGRn7pnVcAAOCrw+tQU1xcrNGjR7tfp6enS5JmzJihtWvXqqKiQuXl5e79DQ0NWrJkiUpLS+Xr66vRo0crPz9f0dHR7pphw4Zp/fr1evrpp/XTn/5UvXr1UnZ2tvr27euuefLJJ1VbW6uHH35Y1dXVuueee7R169Ymd1YBAICvpit6Tk1bwnNqAABoe26o59QAAABcD4QaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKXv+ahLbqwoOTXS5XK88EAABcrguf25fzCxC+MqHm5MmTkqSoqKhWngkAAPDWyZMn5XQ6v7TmK/O7nxobG/XRRx8pKChIDofjmryHy+VSVFSUPvzwQyt/vxTra9tYX9tn+xpZX9t2rdZnjNHJkycVGRkpH58vv2rmK3OmxsfHR926dbsu7xUcHGzlN+wFrK9tY31tn+1rZH1t27VY36XO0FzAhcIAAMAKhBoAAGAFQs1V5O/vr2eeeUb+/v6tPZVrgvW1bayv7bN9jayvbbsR1veVuVAYAADYjTM1AADACoQaAABgBUINAACwAqEGAABYgVDTQs8//7yGDRum9u3bq2PHjs3WlJeXa8KECWrfvr1CQ0P1xBNP6Ny5cx4127dv15133il/f3/17NlTa9euvfaT99L27dvlcDia3Xbv3i1JKisra3b/rl27Wnn2lyc6OrrJ3BcsWOBRs3//fn39619XQECAoqKitGjRolaarXfKysqUnJysmJgYBQYG6rbbbtMzzzyjs2fPetS05eMnScuWLVN0dLQCAgIUFxenoqKi1p5Si2RmZmrIkCEKCgpSaGiokpKSVFpa6lEzatSoJsfqBz/4QSvN2DvPPvtsk7n37t3bvf/MmTNKSUlR586d1aFDB02aNElVVVWtOGPvNPdvicPhUEpKiqS2d+x27NihiRMnKjIyUg6HQ9nZ2R77jTGaN2+eIiIiFBgYqISEBL3//vseNZ9++qmmTZum4OBgdezYUcnJyTp16tS1mbBBi8ybN88sXbrUpKenG6fT2WT/uXPnTN++fU1CQoLZu3ev2bJli+nSpYvJyMhw1/zrX/8y7du3N+np6eadd94xv/71r027du3M1q1br+NKLq2urs5UVFR4bN///vdNTEyMaWxsNMYYc/jwYSPJvPHGGx51Z8+ebeXZX55bb73VPPfccx5zP3XqlHt/TU2NCQsLM9OmTTMlJSXmlVdeMYGBgeZ3v/tdK8768vztb38zM2fONK+//rr54IMPzKZNm0xoaKiZM2eOu6atH7+srCzj5+dnVq9ebQ4cOGBmzZplOnbsaKqqqlp7al5LTEw0a9asMSUlJWbfvn3m3nvvNd27d/f4fhw5cqSZNWuWx7GqqalpxVlfvmeeecZ87Wtf85j7xx9/7N7/gx/8wERFRZnc3FxTXFxshg4daoYNG9aKM/bO8ePHPdaWk5NjJJk333zTGNP2jt2WLVvMz372M/Pqq68aSWbjxo0e+xcsWGCcTqfJzs42//znP839999vYmJizOeff+6uGTdunBkwYIDZtWuXeeutt0zPnj3N1KlTr8l8CTVXaM2aNc2Gmi1bthgfHx9TWVnpblu+fLkJDg42dXV1xhhjnnzySfO1r33No9/kyZNNYmLiNZ3zlTp79qzp2rWree6559xtFz4U9+7d23oTuwK33nqr+eUvf3nR/b/97W9Np06d3MfOGGOeeuopExsbex1md/UtWrTIxMTEuF+39eN39913m5SUFPfrhoYGExkZaTIzM1txVlfH8ePHjSSTl5fnbhs5cqSZPXt2603qCjzzzDNmwIABze6rrq42vr6+ZsOGDe62gwcPGkmmoKDgOs3w6po9e7a57bbb3P8D2JaP3RdDTWNjowkPDzeLFy92t1VXVxt/f3/zyiuvGGOMeeedd4wks3v3bnfN3/72N+NwOMyxY8eu+hz58dM1UlBQoH79+iksLMzdlpiYKJfLpQMHDrhrEhISPPolJiaqoKDgus7VW3/5y1904sQJPfTQQ0323X///QoNDdU999yjv/zlL60wu5ZbsGCBOnfurEGDBmnx4sUePyosKCjQiBEj5Ofn525LTExUaWmpPvvss9aY7hWpqalRSEhIk/a2ePzOnj2rPXv2ePxd8vHxUUJCwg3/d+ly1NTUSFKT4/XHP/5RXbp0Ud++fZWRkaHTp0+3xvRa5P3331dkZKR69OihadOmqby8XJK0Z88e1dfXexzL3r17q3v37m3yWJ49e1Yvv/yyvve973n8IuW2fOz+3eHDh1VZWelxvJxOp+Li4tzHq6CgQB07dtRdd93lrklISJCPj48KCwuv+py+Mr/Q8nqrrKz0CDSS3K8rKyu/tMblcunzzz9XYGDg9Zmsl1588UUlJiZ6/ILQDh06aMmSJRo+fLh8fHz0v//7v0pKSlJ2drbuv//+Vpzt5fnRj36kO++8UyEhIcrPz1dGRoYqKiq0dOlSSeePVUxMjEeffz+enTp1uu5zbqlDhw7p17/+tX7xi1+429ry8fvkk0/U0NDQ7N+ld999t5VmdXU0NjYqLS1Nw4cPV9++fd3t//mf/6lbb71VkZGR2r9/v5566imVlpbq1VdfbcXZXp64uDitXbtWsbGxqqio0Pz58/X1r39dJSUlqqyslJ+fX5PrFMPCwtz/brYl2dnZqq6u1syZM91tbfnYfdGFY9Lc371//5wLDQ312H/TTTcpJCTkmhxTQs2/+clPfqKFCxd+ac3Bgwc9Lmpry1qy3qNHj+r111/Xn//8Z4+6Ll26KD093f16yJAh+uijj7R48eJW+1D0Zn3/Pvf+/fvLz89PjzzyiDIzM2/YR5q35PgdO3ZM48aN03e+8x3NmjXL3X4jHj9IKSkpKikp0c6dOz3aH374YffX/fr1U0REhMaMGaMPPvhAt9122/WeplfGjx/v/rp///6Ki4vTrbfeqj//+c837P/ItdSLL76o8ePHKzIy0t3Wlo9dW0Co+Tdz5szxSNTN6dGjx2WNFR4e3uTuiwtX8IeHh7v/+8Wr+quqqhQcHHxd/nK3ZL1r1qxR586dL+uDLi4uTjk5OVcyxStyJcczLi5O586dU1lZmWJjYy96rKT/O57Xm7fr++ijjzR69GgNGzZMK1euvOT4rX38LleXLl3Url27Zo9Pax2bqyE1NVWbN2/Wjh07PM6KNicuLk7S+bNwbe2DsWPHjrr99tt16NAhfeMb39DZs2dVXV3tcbamLR7LI0eO6I033rjkGZi2fOwuHJOqqipFRES426uqqjRw4EB3zfHjxz36nTt3Tp9++uk1OaaEmn/TtWtXde3a9aqMFR8fr+eff17Hjx93n3rLyclRcHCw7rjjDnfNli1bPPrl5OQoPj7+qszhUrxdrzFGa9as0fTp0+Xr63vJ+n379nl8o19vV3I89+3bJx8fH/exi4+P189+9jPV19e7156Tk6PY2NhW+9GTN+s7duyYRo8ercGDB2vNmjXy8bn05XStffwul5+fnwYPHqzc3FwlJSVJOv9jm9zcXKWmprbu5FrAGKPHHntMGzdu1Pbt25v82LM5+/btk6Q2cby+6NSpU/rggw/03e9+V4MHD5avr69yc3M1adIkSVJpaanKy8uv27+LV8uaNWsUGhqqCRMmfGldWz52MTExCg8PV25urjvEuFwuFRYW6tFHH5V0/t/O6upq7dmzR4MHD5Ykbdu2TY2Nje5Ad1Vd9UuPvyKOHDli9u7da+bPn286dOhg9u7da/bu3WtOnjxpjPm/W7rHjh1r9u3bZ7Zu3Wq6du3a7C3dTzzxhDl48KBZtmzZDXlL9wVvvPGGkWQOHjzYZN/atWvN+vXrzcGDB83BgwfN888/b3x8fMzq1atbYabeyc/PN7/85S/Nvn37zAcffGBefvll07VrVzN9+nR3TXV1tQkLCzPf/e53TUlJicnKyjLt27dvE7d0Hz161PTs2dOMGTPGHD161ONW0gva8vEz5vwt3f7+/mbt2rXmnXfeMQ8//LDp2LGjx92HbcWjjz5qnE6n2b59u8exOn36tDHGmEOHDpnnnnvOFBcXm8OHD5tNmzaZHj16mBEjRrTyzC/PnDlzzPbt283hw4fNP/7xD5OQkGC6dOlijh8/bow5f0t39+7dzbZt20xxcbGJj4838fHxrTxr7zQ0NJju3bubp556yqO9LR67kydPuj/fJJmlS5eavXv3miNHjhhjzt/S3bFjR7Np0yazf/9+88ADDzR7S/egQYNMYWGh2blzp+nVqxe3dN9oZsyYYSQ12S48i8AYY8rKysz48eNNYGCg6dKli5kzZ46pr6/3GOfNN980AwcONH5+fqZHjx5mzZo113chXpg6depFnxexdu1a06dPH9O+fXsTHBxs7r77bo/bMm9ke/bsMXFxccbpdJqAgADTp08f89///d/mzJkzHnX//Oc/zT333GP8/f3NLbfcYhYsWNBKM/bOmjVrmv1e/ff/p2nLx++CX//616Z79+7Gz8/P3H333WbXrl2tPaUWudixuvBvQ3l5uRkxYoQJCQkx/v7+pmfPnuaJJ564oZ918u8mT55sIiIijJ+fn7nlllvM5MmTzaFDh9z7P//8c/PDH/7QdOrUybRv395885vf9AjgbcHrr79uJJnS0lKP9rZ47N58881mvx9nzJhhjDl/W/fcuXNNWFiY8ff3N2PGjGmy7hMnTpipU6eaDh06mODgYPPQQw+5TwBcbQ5jjLn6538AAACuL55TAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAV/j+2+FkiJeyAkAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Reference 1 problem solving\n",
        "p=1\n",
        "a=95\n",
        "b=100\n",
        "for i in range(3):\n",
        "    a = a - i\n",
        "    b = b - i\n",
        "    p *= a / b\n",
        "\n",
        "\n",
        "# Reference 2 concavity\n",
        "domain = np.arange(-101, 101)\n",
        "def sudo_function(X):\n",
        "    return np.square(X) + 2*X + 0\n",
        "def sudo_func_dx(X):\n",
        "    return 2*X + 2\n",
        "def sudo_func_dx_dx(X):\n",
        "    return np.full(shape=X.shape, fill_value=2)\n",
        "\n",
        "Y = sudo_function(domain)\n",
        "Y_dx = sudo_func_dx(domain)\n",
        "Y_dx_dx = sudo_func_dx_dx(domain)\n",
        "# If second order derivative is positive for all x in domain then the function is concave, ie\n",
        "# the change in gradient is positive\n",
        "plt.plot(domain, Y_dx_dx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OGP8jDnAWO-"
      },
      "source": [
        "\n",
        "\n",
        "The code was originally written on Colab and run on its default GPUs.\n",
        "\n",
        "If you are running it on some terminal where maybe torchvision isn't installed then execute the following lines first,\n",
        "\n",
        "```\n",
        "source /etc/anaconda3/bin/activate\n",
        "pip install torchvision\n",
        "anaconda-navigator\n",
        "\n",
        "```\n",
        "\n",
        "The above will install torchvision and open the navigator, from which you can select Jupyter and load this file into it - and have fun :)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6U3Xeo_PjVP"
      },
      "source": [
        "# Introduction to VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HquAT91SNrit"
      },
      "source": [
        "\n",
        "\n",
        "Suppose we have two random variables $\\vec{x}$ and $\\vec{z}$ (hence there is a joint distribution over them) and consider being given two conditional distributions $q(\\vec{z}|\\vec{x})$ and $p(\\vec{z}|\\vec{x})$ both of which are trying to approximate the true conditional distribution of $\\vec{z}$ given $\\vec{x}$. Then we can compute the KL divergence between the two to try to measure the discrepancy between these two descriptions. Towards that we observe the following analysis of this measure of discrepancy (we do an abuse of notation here to use $\\vec{z}$ for denoting two different things - in the LHS its labelling the distributions and in the RHS its indexing all possible samples of it),\n",
        "\n",
        "\\begin{align}\n",
        "&{\\rm KL}(q(\\vec{z}|\\vec{x})||p(\\vec{z}|\\vec{x}))\\\\\n",
        "&= \\sum_{\\vec{z}} q(\\vec{z}|\\vec{x}) \\log \\frac{q(\\vec{z}|\\vec{x})}{p(\\vec{z}|\\vec{x})}\\\\\n",
        "&= -\\sum_{\\vec{z}} q(\\vec{z}|\\vec{x}) \\log \\frac{p(\\vec{z}|\\vec{x})}{q(\\vec{z}|\\vec{x})}\\\\\n",
        "&= -\\sum_{\\vec{z}} q(\\vec{z}|\\vec{x})  \\left[\\log p(\\vec{z}|\\vec{x}) - \\log q(\\vec{z}|\\vec{x}) \\right]\\\\\n",
        "&= -\\sum_{\\vec{z}} q(\\vec{z}|\\vec{x}) \\left[\\log \\frac{p(\\vec{x}|\\vec{z})p(\\vec{z})}{p(\\vec{x})} - \\log q(\\vec{z}|\\vec{x}) \\right]\\\\\n",
        "&= -\\sum_{\\vec{z}} q(\\vec{z}|\\vec{x}) \\left[\\log p(\\vec{x}|\\vec{z}) + \\log p(\\vec{z}) - \\log p(\\vec{x}) - \\log q(\\vec{z}|\\vec{x}) \\right]\\\\\n",
        "&= \\sum_{\\vec{z}} q(\\vec{z}|\\vec{x}) \\left[-\\log p(\\vec{x}|\\vec{z}) - \\log p(\\vec{z}) + \\log p(\\vec{x}) + \\log q(\\vec{z}|\\vec{x}) \\right]\n",
        "\\end{align}\n",
        "\n",
        "The above can be written as,  \n",
        "\n",
        "\\begin{align}\n",
        "\\log p(\\vec{x}) = &{\\rm KL}(q(\\vec{z}|\\vec{x})||p(\\vec{z}|\\vec{x})) - \\sum_{\\vec{z}} q(\\vec{z}|\\vec{x}) \\left[-\\log p(\\vec{x}|\\vec{z}) - \\log p(\\vec{z}) + \\log q(\\vec{z}|\\vec{x}) \\right]\\\\\n",
        "= &{\\rm KL}(q(\\vec{z}|\\vec{x})||p(\\vec{z}|\\vec{x})) + \\underbrace{\\left[ \\mathbb{E}_{q} \\log p(\\vec{x}|\\vec{z})  - \\mathbb{E}_{q}  \\log \\frac{q(\\vec{z}|\\vec{x})}{p(\\vec{z})} \\right]}_{\\bf ELBO = -V.F.E}\\\\\n",
        "\\end{align}\n",
        "\n",
        ">The above argument was done assuming that the $\\sum_{\\vec{z}}$ makes sense and that necessarily needs a finite number of possible values for the random variable $\\vec{z}$. But we shall skip re-writing the above proof for continuous random variables - as needed in this experiment - and shall just assert without proof the veracity of the last equality obtained.\n",
        "\n",
        "The second term on the LHS is called the \"Evidence Lower Bound (ELBO)\" -- since because of the non-negativity of the KL (the so-called \"variational gap\") we can write,\n",
        "\n",
        "$$ {\\rm ELBO} \\leq \\log p(\\vec{x})$$\n",
        "\n",
        "\n",
        "Since $\\log p(\\vec{x})$ is a constant, and we want to minimize the ${\\rm KL}$ term, we can achieve the same by maximizing the ELBO.\n",
        "Which is the same as the minimization of its negative i.e the optimization problem that we actually code for is,\n",
        "\n",
        "\n",
        "$$\n",
        "\\min \\Bigg ( \\underbrace{\\underbrace{- \\color{blue}{\\mathbb{E}_{q} \\log p(\\vec{x}|\\vec{z})}}_{{\\rm ``Reconstruction ~Loss\"}}  + \\color{red}{ {\\rm KL} (  q(\\vec{z}|\\vec{x}) ||  p(\\vec{z})) }}_{``\\textbf{Variational Free Energy (V.F.E)}\"} \\Bigg )\n",
        "$$\n",
        "\n",
        "\n",
        "Thus by minimizing the VFE we bring the $p(\\vec{z}|\\vec{x})$ to be close to the $q(\\vec{z}|\\vec{x})$ -- thus the process of minimizing the V.F.E. can be seen as an attempt to bring close two different parameterizations of the conditional distribution of $\\vec{z}$ given $\\vec{x}$. ***We shall see that the two nets to be eventually used essentially play this role of giving these two parameterizations.***\n",
        "\n",
        "**BUT note that its $p(\\vec{x}|\\vec{z})$ that occurs in the optimization objective** -- and hence at the end of the training a good $p(\\vec{x}|\\vec{z})$ will be read off and that in turn reveals the ``unknown\" distribution $p(\\vec{x})$ via the formula $p(\\vec{x}) = \\int p(\\vec{x}|\\vec{z}) p(\\vec{z}) d\\vec{z}$. **We are very crucially assuming that the last integral here is doable which is what will motivate us to choose simple distributions for $\\vec{z}$.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf7LVYpMh0V-"
      },
      "source": [
        "\n",
        "##Objective\n",
        "\n",
        "> Given samples $\\vec{x} \\sim p(\\vec{x})$ we want to uncover the function $p(\\vec{x})$ by minimizing the V.F.E over the three distributions that occur in it - which we recall are given in terms of,\n",
        "\n",
        "> (a) $p(\\vec{x} \\vert \\vec{z})$, the \"posterior distribution\" w.r.t $\\vec{z}$ the ``fictitious\" latent r.v\n",
        "\n",
        "> (b) $q(\\vec{z} \\vert \\vec{x})$ and\n",
        "\n",
        "> (c) $p(\\vec{z})$ the distribution of the \"latent variable\"\n",
        "\n",
        "Setting the above objective is essentially tantamount of assuming that the data $\\vec{x}$ are indeed samples of a distribution $p(\\vec{x})$ which can be obtained as the marginal of a ***\"generative model\"*** which is a joint distirbution $p(\\vec{x},\\vec{z})$ over the data $\\vec{x}$ and the latent variable $\\vec{z}$. This assumption of the factorization \"$p(\\vec{z},\\vec{x}) = p(\\vec{x} \\vert \\vec{z}) \\cdot p(\\vec{z})$\" of this imagined joint distribution over the data and a fictitious latent variable is a key insight that gets the entire field started.\n",
        "\n",
        "\n",
        "Towards achieving the aforesaid objective we recall the following fact about Gaussian distribution and note two key ideas.\n",
        "\n",
        "---\n",
        "FACTS :  \n",
        "\n",
        "- Given a $d-$dimensional vector $\\vec{\\mu}$ and a $d-$dimensional invertible symmetric matrix $\\Sigma$, We use the following notation to denote the multi-variable Gaussian distribution's p.d.f.,\n",
        "\n",
        "$${\\cal N}(\\vec{\\mu},\\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^d {\\rm det}(\\Sigma)}} e^{-\\frac{1}{2} (\\vec{x}-\\vec{\\mu})^\\top \\Sigma^{-1} (\\vec{x}-\\vec{\\mu})}  $$\n",
        "\n",
        "- If $\\vec{X} \\sim {\\cal N}(\\vec{\\mu},\\Sigma)$ as above, then it is equivalent to assuming that $\\vec{a}^\\top \\vec{X}$ is a one dimensional Gaussian distributed random variable for every vector $\\vec{a}$ in $d-$dimensions. This gives a more intutive picture of what multivariate Gaussian distributions look like.\n",
        "\n",
        "- Recall that if $\\vec{X} \\sim {\\cal N}(\\vec{\\mu},\\Sigma)$ then, $a\\vec{X} + \\vec{b} \\sim {\\cal N}(a\\vec{\\mu} + \\vec{b},a^2\\Sigma)$\n",
        "\n",
        "  In particular, $\\vec{a}{\\cal N}(0,I) + \\vec{b} \\sim  {\\cal N}( \\vec{b},{\\rm diag}(a^2))$\n",
        "\n",
        "  -- this particular case shall be needed very soon!\n",
        "\n",
        "\n",
        "- Recall ([reference ](https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/)) that we have an expression for exact KL diverergence between two Gaussians in the same dimension. If we have two random variables in $k$ dimensions, $f_i \\sim {\\cal N}(\\vec{\\mu_i},\\Sigma_i)$ for $i=1,2$ then we have,\n",
        "\n",
        "$${\\rm KL}(f_1 || f_2) = \\frac{1}{2} \\cdot \\left [-k + (\\vec{\\mu_1} - \\vec{\\mu_2})^\\top \\Sigma_2^{-1} (\\vec{\\mu_1} - \\vec{\\mu_2}) +  \\log \\frac{\\det (\\Sigma_2)}{\\det (\\Sigma_1)} + {\\rm Tr} (\\Sigma_2^{-1}\\Sigma_1) \\right ]$$\n",
        "\n",
        "$$\\text{And in particular,}$$\n",
        "\n",
        "$${\\rm KL}({\\cal N}(\\vec{\\mu},\\Sigma) || {\\cal N}(0,I) ) =  \\frac{1}{2} \\cdot \\left [-k + \\Vert{\\vec{\\mu}}\\Vert^2 + {\\rm Tr} (\\Sigma) - \\log {\\det (\\Sigma)} \\right ] $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf8_AhnBRPR7"
      },
      "source": [
        "##Solution Ideas\n",
        "\n",
        "\n",
        "- The **first key idea** will be exploit the above identity and to model $p(z), p(x \\vert z)$ and $q(z | x)$ as Gaussian distributions.\n",
        "\n",
        "\n",
        "- The **second key idea**, is that we will try to make the Gaussian distribution $q(z |x)$ as expressive as possible by using a neural net (the \"Encoder'') to represent its mean and variance and we will have a second neural net (the \"Decoder\") to represent the mean of $p(x \\vert z)$.\n",
        "\n",
        "In the next subsection we define these two nets.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4oV3JYdaDLs"
      },
      "source": [
        "\n",
        "##Defining The Nets\n",
        "\n",
        "Assuming that $x \\in \\mathbb{R}^{\\rm image-dimension}$, we *choose* an appropriate value of ${\\rm latent-dimension}$ s.t $z \\in \\mathbb{R}^{\\rm latent-dimension}$. Then the needed pair of nets can be denoted as,  \n",
        "    \n",
        "\n",
        "$$ {\\rm Encoder}_{\\Phi} : \\mathbb{R}^{\\rm image-dimension} \\rightarrow \\mathbb{R}^{\\rm latent-dimension} \\times \\mathbb{R}^{\\rm latent-dimension} $$\n",
        "\n",
        "$$ {\\rm Decoder}_{\\Theta} : \\mathbb{R}^{\\rm latent-dimension} \\rightarrow \\mathbb{R}^{\\rm image-dimension} $$\n",
        "      \n",
        "  In above $\\Phi$ and $\\Theta$ are to denote the set of all trainable weights in the Encoder and the Decoder respectively. In the particular implementation that will demonstrate here we shall have both these nets be of depth $3$ (and hence each of them will have $3$ affine transforms). The first two activation layers in each would be ${\\rm hidden-dimension}$ sized ${\\rm Leaky-ReLU}$ activations. The Encoder has 2 layers of activation while the Decoder will be using a third layer of sigmoid activations at the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5uSufJfTpPk"
      },
      "source": [
        "##Parameterizing The Distributions\n",
        "\n",
        "Given any data sample $\\vec{x}$, we define the mean $\\vec{\\mu}$ and the diagonal variances $\\vec{\\sigma}$ of the assumed Gaussian, $q(\\vec{z}|\\vec{x})$ as,\n",
        "\n",
        "- $q(\\vec{z} |\\vec{x}) = q_{\\Phi}(\\vec{z} |\\vec{x}) :=  {\\cal N}(f({\\rm Encoder}_{\\Phi} (\\vec{x})))$\n",
        "\n",
        "Where $f$ is a transformation -- that we shall soon specify -- which we shall do to the outputs of the ${\\rm Encoder}_\\Phi$ to obtain the mean and the variance of the Normal distribution of $q(\\vec{z} | \\vec{x})$.\n",
        "\n",
        "Given any sample $\\vec{z}$ from $q_{\\Phi}(\\vec{z} |\\vec{x})$, we let the $p$ be more general and only assume that,\n",
        "\n",
        "- $p(\\vec{x} |\\vec{z}) = p_{\\Theta}(\\vec{x} |\\vec{z}) := \\mathcal{N}({\\rm Decoder}_{\\Theta}(\\vec{z}), {\\rm diag(all{-}ones{-}vector)})$\n",
        "\n",
        "We assume that the distribution of the latent variable is just the standard normal,\n",
        "\n",
        "- $p(\\vec{z}) = {\\cal N}(0,I)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rr4oY97gxiM"
      },
      "source": [
        "##The V.A.E.\n",
        "\n",
        "If we can sample $\\vec{z_s} \\sim q_{\\Phi}(\\vec{z}|\\vec{x}) ~\\forall s=1,\\ldots,S$  then the  minimization objective -- the \"**Variational Free Energy (V.F.E.)**\" is replaced by its empirical (Monte Carlo) estimate,\n",
        "\n",
        "\\begin{align*}\n",
        "{\\rm Empirical ~V.F.E.} :=&- \\color{blue}{\\tilde{\\mathbb{E}}_{q_{\\Phi}} \\log p_{\\Theta}(\\vec{x}|\\vec{z})}  + \\color{red}{ \\tilde{\\mathbb{E}}_{q_{\\Phi}} \\log \\left (  \\frac{q_\\Phi(\\vec{z}|\\vec{x})}{p(\\vec{z})} \\right ) }\\\\\n",
        "= & \\frac{1}{S} \\sum_{s=1}^S \\left \\{ \\underbrace{\\color{blue}{- \\log p_{\\Theta}(\\vec{x}|\\vec{z_s})}}_{\\rm Reconstuction Loss} + \\underbrace{\\color{red} { \\log q_\\Phi(\\vec{z_s}|\\vec{x}) - \\log p(\\vec{z_s})}}_{\\rm KL-Divergence}  \\right \\}\\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\n",
        "We note that, ``Variational Autoencoder (VAE)\" is not a single architecture but a system which uses 2 neural nets for implementing the plan as said above for learning an unknown marginal distribution of the data $p(x)$ by approximating it as $p_{\\Theta} (\\vec{x}) := \\int p_{\\Theta} (\\vec{x} | \\vec{z}) p(\\vec{z})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZ-E3j8PbJ-"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSCHKEnHD162",
        "outputId": "8a74b241-14eb-4ec7-c19a-3a3a2ce0eecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 10 17:29:20 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M9GK0NbeTlNd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#from torchinfosummary import summary (sometimes this works)\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FomJElr6gAwB"
      },
      "source": [
        "#Model and Optimization Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZlzafzGJ6_a"
      },
      "source": [
        "You can speed-up this code almost 3x if you run it on Google Colab\n",
        "\n",
        "--  and change settings as, Runtime ->Change runtime type ->Hardware Accelerator -> GPU\n",
        "\n",
        "-- and then set cuda=True below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HfK6KzjwfygZ"
      },
      "outputs": [],
      "source": [
        "dataset_path = '~/datasets'\n",
        "\n",
        "cuda = True\n",
        "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "x_dim  = 784\n",
        "#x_dim will be used as the value for the input_dim and output_dim below and it is the same as the image-dimension in the text above\n",
        "hidden_dim = 1024\n",
        "latent_dim = 8\n",
        "\n",
        "lr = 1e-3\n",
        "epochs = 20\n",
        "#use number of epoch = something which is 1 + a multiple of 5\n",
        "# - that keeps it consistent with the fact that the population risk is computed every 5 epochs.\n",
        "train_size = 60000\n",
        "#also try 60000\n",
        "validation_size = 60000 - train_size\n",
        "batch_size = 10**2\n",
        "test_batch_size = 10**4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP89FRN6JOgx"
      },
      "source": [
        "Check versions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ4zApvQJQfp",
        "outputId": "1cd403df-561b-4f28-ee12-76d145fd4ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "1.26.4\n",
            "0.20.1+cu121\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(np.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVKE4YRpgrFo"
      },
      "source": [
        "#The Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MQggpVDmgn_C"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import MNIST, FashionMNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "mnist_transform = transforms.Compose([transforms.ToTensor(),])\n",
        "#ToTensor converts the  data images to torch tensor\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "#The above is some setting with the GPUs\n",
        "\n",
        "train_dataset = FashionMNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n",
        "test_dataset  = FashionMNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n",
        "\n",
        "train_dataset, validation_dataset = random_split(train_dataset, [train_size, validation_size])\n",
        "\n",
        "#The above line redefines the train_dataset to use a randomly chosen train_size sized subset of the train_dataset\n",
        "#Quite obviously using larger train_size should help.\n",
        "#In this particular demonstration we shall not make use of the validation_dataset that gets produced above.\n",
        "#Typically performance on the validation_dataset is what is used to determine good choices of achitecture and the step-lengths and the number of epochs.\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader  = DataLoader(dataset=test_dataset,  batch_size=test_batch_size, shuffle=False, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L5g-QhSiSrr"
      },
      "source": [
        "For various future purposes it is important to know that both the data options we work with above consist of 60000 training data and 10000 test data, and all being 28x28 black and white images. These 28x28 pixel values range from 0 to 255 - the smaller numbers closer to zero represent the darker shade while the larger numbers closer to 255 represent the lighter or the white shade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of02NKU9kvxI",
        "outputId": "fe7c1b22-b0e8-4972-ee2b-352499bcdc99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "test_size = len(test_dataset)\n",
        "print(train_size)\n",
        "print(test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1Ge4Zu139hN"
      },
      "source": [
        "#Defining Our Variational AutoEncoder (VAE)\n",
        "\n",
        "NOTATION : Any quantity like $A_{\\rm some-number}$ or $B_{\\rm some-number}$ denotes an affine transformation between appropriate Euclidean spaces.  These are implemented in PyTorch via the keyword ``nn.Linear\" - quite a misnomer!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq1lJMVT-ghG"
      },
      "source": [
        "##Defining The Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PTJHiU5Gi1BZ"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.FC_layer_1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.FC_layer_2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
        "\n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_1      = self.LeakyReLU(self.FC_layer_1(x))\n",
        "        # R^{image_dim} \\ni x -> = LeakyReLU(A_1(x)) = h_1 \\in R^{hidden_dim}\n",
        "        h_2       = self.LeakyReLU(self.FC_layer_2(h_1))\n",
        "        # R^{hidden_dim} \\ni h_1 -> LeakyReLU(A_2(h_1)) = h_2 \\in R^{hidden_dim}\n",
        "        mean     = self.FC_mean(h_2)\n",
        "        log_var  = self.FC_var(h_2)\n",
        "        # R^{hidden_dim} \\ni h_2 -> (A_31(h_2),A_32(h_2)) = (mean,log_var) \\in R^{latent_dim} x R^{latent_dim}\n",
        "\n",
        "        # encoder produces mean and log of variance i.e., parameters of a Gaussian distribution \"q\"\n",
        "\n",
        "        return mean, log_var\n",
        "\n",
        "# So here the Encoder is a depth-3 net mapping R^{input_dim} -> R^{latent_dim} x R^{latent_dim} as x -> ( A31( ReLU(A2( ReLU(A1(x)) )) ), A32( ReLU(A2( ReLU(A1(x)) )) ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMjFVRs46RTM"
      },
      "source": [
        "We define the following equivalences betwee the program variables and the mathematical variable,\n",
        "\n",
        "${\\tt input\\_dim} = {\\rm image-dimension}$\n",
        "\n",
        "${\\tt hidden\\_dim} = {\\rm hidden-dimension}$\n",
        "\n",
        "${\\tt latent\\_dim} = {\\rm latent-dimension}$\n",
        "\n",
        "\n",
        "Then the structure of the encoder defined above motivates the definintion of two sub-networks of the ${\\rm Encoder}$ net as ${\\rm Enc}_1$ and ${\\rm Enc}_2$ to facilitate future description :\n",
        "\n",
        "\\begin{align}\n",
        "{\\rm Encoder} : \\mathbb{R}^{\\rm image-dimension} \\rightarrow &\\mathbb{R}^{\\rm latent-dimension} \\times \\mathbb{R}^{\\rm latent-dimension} \\\\\n",
        "\\vec{x} \\mapsto &({\\rm Enc}_1 (\\vec{x}),{\\rm Enc}_2 (\\vec{x}))\n",
        "= (A_{31}(\\vec{h_2}),A_{32}(\\vec{h_2})) = ({\\rm mean}, {\\rm log-var})\\\\\n",
        "&{\\tt {\\rm where , } ~\\mathbb{R}^{\\rm hidden-dimension} \\ni \\vec{h_2} = {\\rm ReLU}( A_2 ({\\rm ReLU} (A_1(\\vec{x}))))}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "Recall that any affine transform of the form $A_* : \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ acts on an input vector $\\vec{q}$ as $A_*(\\vec{q}) = L_*(\\vec{q}) + \\vec{b}_*$ where $\\vec{b}_* \\in \\mathbb{R}^n$ and $L_*$ is a $m \\times n$ matrix. **Question :** In terms of ${\\rm input/output/hidden - dimension}$ count how many trainable parameters does the encoder have?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Py0lEMDAhY8",
        "outputId": "933b0a91-5cac-43fb-b632-09df7db4f1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=784, out_features=1024, bias=True) Trainable Parameters: 803,840\n",
            "Linear(in_features=1024, out_features=1024, bias=True) Trainable Parameters: 1,049,600\n",
            "Linear(in_features=1024, out_features=8, bias=True) Trainable Parameters: 8,200\n",
            "Linear(in_features=1024, out_features=8, bias=True) Trainable Parameters: 8,200\n",
            "Total parameters: 1,869,840\n"
          ]
        }
      ],
      "source": [
        "# This is the parameter count code, it takes as input a model\n",
        "# and outputs the parameters in each layer and the total number\n",
        "# The general formula will be (input_dim * output_dim) + output_dim\n",
        "# (input_dim * output_dim) gives the number of weights\n",
        "# + output_dim arises due to the application of a bias term.\n",
        "def parameter_count(model):\n",
        "    total_parameters = 0\n",
        "    layer_parameters = []\n",
        "    layer_type = []\n",
        "    for layer in model.children():\n",
        "        i_feature = getattr(layer, 'in_features', None)\n",
        "        o_feature = getattr(layer, 'out_features', None)\n",
        "\n",
        "        # I believe all layers should have both of these features\n",
        "        if i_feature is None or o_feature is None:\n",
        "            continue\n",
        "        parameters = (i_feature * o_feature) + o_feature\n",
        "\n",
        "        layer_parameters.append(parameters)\n",
        "        total_parameters += parameters\n",
        "        layer_type.append(layer)\n",
        "\n",
        "    [print(name, 'Trainable Parameters:', f'{x:,}') for x, name in zip(layer_parameters, layer_type)]\n",
        "    print('Total parameters:', f'{total_parameters:,}')\n",
        "\n",
        "    return layer_parameters, total_parameters\n",
        "\n",
        "# Here the model has not been defined yet, but given that\n",
        "\n",
        "layer_parameters, total_parameters = parameter_count(Encoder(x_dim, hidden_dim, latent_dim))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h33KnZvl-p9-"
      },
      "source": [
        "## Defining The Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jj8_ocPpi5FG"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.FC_dec_layer_1 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.FC_dec_layer_2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, z):\n",
        "        dec_h_1     = self.LeakyReLU(self.FC_dec_layer_1(z))\n",
        "        # R^{latent_dim} \\ni z -> ReLU(B1(z)) = dec_h_1 \\in R^{hidden_dim}\n",
        "\n",
        "        dec_h_2     = self.LeakyReLU(self.FC_dec_layer_2(dec_h_1))\n",
        "        # R^{hidden_dim} \\ni dec_h_1 -> ReLU(B2(dec_h_1)) = dec_h_2 \\in R^{hidden_dim}\n",
        "\n",
        "        x_hat = torch.sigmoid(self.FC_output(dec_h_2))\n",
        "        #R^{hidden_dim} \\ni dec_h_2 -> Sigmoid(B3(dec_h_2)) = x_hat \\in R^{output_dim}\n",
        "\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "# So here the Decoder is a depth-3 net mapping R^{latent_dim} -> R^{output_dim} as z -> Sigmoid(B3( ReLU(B2( ReLU(B1(z)) )) ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B31tHJY1rhnt"
      },
      "source": [
        "We further define the following renaming between the program variables and the mathematical variable,\n",
        "\n",
        "${\\tt output\\_dim} = {\\rm decoder-output-dimension}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Recall that any affine transform of the form $A_* : \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ acts on an input vector $\\vec{q}$ as $A_*(\\vec{q}) = L_*(\\vec{q}) + \\vec{b}_*$ where $\\vec{b}_* \\in \\mathbb{R}^n$ and $L_*$ is a $m \\times n$ matrix. **Question :** In terms of ${\\rm input/output/hidden - dimension}$ count how many trainable parameters does the decoder have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF6nIbZWAhY8",
        "outputId": "caad6258-352a-4942-9cc8-81e265976af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=8, out_features=1024, bias=True) Trainable Parameters: 9,216\n",
            "Linear(in_features=1024, out_features=1024, bias=True) Trainable Parameters: 1,049,600\n",
            "Linear(in_features=1024, out_features=784, bias=True) Trainable Parameters: 803,600\n",
            "Total parameters: 1,862,416\n"
          ]
        }
      ],
      "source": [
        "layer_parameters, total_parameters = parameter_count(Decoder(latent_dim, hidden_dim, x_dim))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5ZalDKQ-yR-"
      },
      "source": [
        "## Combining The Encoder And The Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nocVfz19i_vO"
      },
      "outputs": [],
      "source": [
        "def reparameterization(mean, var):\n",
        "        epsilon = torch.randn_like(var).to(DEVICE)\n",
        "        # sampling epsilon ~ N(0,I_{latent-dimension x latent-dimension})\n",
        "        y = mean + var*epsilon\n",
        "        # The so-called \"reparameterization trick\"\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUwn9lhzqM5v"
      },
      "source": [
        "The above function specifies that $\\vec{y}$ is a random variable s.t,\n",
        "$${\\mathbb{R}}^{{\\rm latent-dimension}} \\ni {\\vec y} \\sim \\vec{ {\\rm mean}} + {\\rm var} \\times {\\cal N}(0,I)) \\sim {\\cal N}(\\vec{{\\rm mean}}, {\\rm diag(var^2)})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KO03r8d34Jh1"
      },
      "outputs": [],
      "source": [
        "## Now we define the final model\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, Encoder, Decoder):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.Encoder = Encoder\n",
        "        self.Decoder = Decoder\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean, log_var = self.Encoder(x)\n",
        "        y = reparameterization(mean, torch.exp(0.5 * log_var))\n",
        "        x_hat = self.Decoder(y)\n",
        "\n",
        "        return x_hat, mean, log_var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92X8OhG5uqS6"
      },
      "source": [
        "NOTE: In here we shall use the $\\log$ and the $\\exp$ to be acting coordinate-wise when applied to a vector.\n",
        "\n",
        "For every input (image) vector $\\vec{x}$, the ``forward\" function above does the following $3$ step computation,  \n",
        "\n",
        "> STEP-1 : $$ (\\vec{\\mu_z}, 2\\cdot \\log(\\vec{\\sigma_z})) := {\\rm Encoder}_{\\Phi} (\\vec{x})$$\n",
        "\n",
        "The \"reparameterization\" function is getting called with ${\\tt mean} = \\vec{\\mu_z}$ and ${\\tt var} = \\exp({\\frac{1}{2} \\times 2\\cdot \\log(\\vec{\\sigma_z})}) = \\vec{\\sigma_z} $\n",
        "\n",
        "\n",
        "And it returns a sample of a random variable s.t,\n",
        "> STEP-2 : $${\\vec z} \\sim   {\\cal N}(\\vec{\\mu_z},{\\rm diag}(\\vec{\\sigma_z} \\circ \\vec{\\sigma_z}))$$\n",
        "\n",
        "(There is a bit of a misnaming here since the word ${\\tt var}$ might remind one of \"variance\" but as is clear from above its actually the ``standard deviation'' of the latent variable. But we shall stick with usual definitions for now.)\n",
        "\n",
        "Also the above 2 Steps together now effectively specify a choice of the function $f$ that was mentioned in the previous section, \"Parameterizing The Distributions\"\n",
        "\n",
        "\n",
        ">STEP-3 : $$\\hat{x} = {\\rm Decoder}_{\\Theta}(\\vec{z})$$\n",
        "\n",
        "Thus for every data $\\vec{x}$ the ${\\tt forward}$ function returns a $3-$tuple of vectors,\n",
        "$${\\tt (x\\_{hat}, mean, log\\_var)} :=  (\\hat{x},\\vec{\\mu_z}, 2\\cdot \\log(\\vec{\\sigma_z}))$$\n",
        "\n",
        "\n",
        "\n",
        "**In many usual neural net implementaions the ``forward\" function/propagation just computes an output vector after evaluating some neural net at the given input. But in V.A.E. things are much more complicated! Here the forward function implements a \"stochastic process'' which uses (two) neural nets, the input data and a source of randomization and computes a set of 3 vectors as given above - of which $\\hat{x}$ is a random variable.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3uM_R01AhRt",
        "outputId": "03565a30-dca3-43fe-f55a-592c11be0341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1              [-1, 1, 1024]         803,840\n",
            "         LeakyReLU-2              [-1, 1, 1024]               0\n",
            "            Linear-3              [-1, 1, 1024]       1,049,600\n",
            "         LeakyReLU-4              [-1, 1, 1024]               0\n",
            "            Linear-5                 [-1, 1, 8]           8,200\n",
            "            Linear-6                 [-1, 1, 8]           8,200\n",
            "           Encoder-7   [[-1, 1, 8], [-1, 1, 8]]               0\n",
            "            Linear-8              [-1, 1, 1024]           9,216\n",
            "         LeakyReLU-9              [-1, 1, 1024]               0\n",
            "           Linear-10              [-1, 1, 1024]       1,049,600\n",
            "        LeakyReLU-11              [-1, 1, 1024]               0\n",
            "           Linear-12               [-1, 1, 784]         803,600\n",
            "          Decoder-13               [-1, 1, 784]               0\n",
            "================================================================\n",
            "Total params: 3,732,256\n",
            "Trainable params: 3,732,256\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.07\n",
            "Params size (MB): 14.24\n",
            "Estimated Total Size (MB): 14.31\n",
            "----------------------------------------------------------------\n",
            "Total number of parameters:  3732256\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
        "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
        "\n",
        "model = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)\n",
        "\n",
        "#(en)/(de)coder is an object of the class (En)/(De)coder\n",
        "#model is an object of the class Model  - which takes as input entire objects encoder and the decoder\n",
        "\n",
        "summary(model,(1,x_dim))\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total number of parameters: \", total_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1odt2Hz8xzKs"
      },
      "source": [
        "The above instantiation of the VAE model crucially forced the following equuality between the different parameters,\n",
        "\n",
        "${\\rm image-dimension} (={\\tt input\\_dim})= {\\rm decoder-output-dimension}(={\\tt output\\_dim}) = {\\tt x\\_dim}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTMp-ld7Dgsn"
      },
      "source": [
        ">This forcing of the input to the encoder and the output of the decoder to be the same dimension motivates the terminology of an ``autoencoder\" - though **strictly speaking an \"autoencoder\" refers to a single neural net mapping $\\mathbb{R}^k \\rightarrow \\mathbb{R}^k$ for some $k$ to be an autoencoder**. *V.A.E is not an autoencoder in the same sense* since it is **NOT** representatable as a single function from the input to the encoder to the outoput of the decoder.*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZgO0s6HKWRz"
      },
      "source": [
        "# Defining the Loss Function and the Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EZKULh3zKbc5"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "# p(x|z) = N(x; x_hat, 1); x_hat = decoder(z)\n",
        "def loss_function(x, x_hat, mean, log_var):\n",
        "    MSE_Loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
        "    KLD      = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - (log_var).exp())\n",
        "    # For dim = 1, torch.sum(a_ij, dim=1) = \\sum_{j=1}^d a_ij\n",
        "    # 1 here is an all ones matrix of size (minibatch_size, latent_dimension)\n",
        "    return MSE_Loss + KLD\n",
        "\n",
        "\n",
        "#optimizer = Adam(model.parameters(), lr=lr)\n",
        "optimizer = Adam(model.parameters(), lr=lr, weight_decay=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkeBUNwEsU3s"
      },
      "source": [
        "Recall the variable ${\\tt batch\\_size}$(say $=m$) that was defined earlier. Now we realize that this $m$ is the \"mini-batch\" size i.e the number of samples of data the eventual algorithm will use to define a stochastic gradient. Further recalling the definition of the ${\\rm image-dimension}$ (the input dimension to the encoder) we realize that the function ${\\rm loss\\_function}$ is designed to get as input as set of $4$ matrices,\n",
        "- $2$ matrices ${\\bf \\tt x}, {\\bf \\tt x\\_{\\rm hat}} \\in \\mathbb{R}^{m \\times {\\rm image-dimension}}$\n",
        "- $2$ matrices, ${\\tt mean, log\\_var} \\in \\mathbb{R}^{m \\times {\\rm latent-dimension}}$\n",
        "\n",
        "**Although the above are the right matrix dimensions for how the code operates, for ease of exposition, in what follows we shall treat ${\\tt (x\\_{hat}, mean, log\\_var)}$ as just vectors of appropriate dimensions as would be obtained from a single input data/image data $\\vec{x}$**\n",
        "\n",
        "Towards understanding the above loss function computation recall the identification that we had established between the theoretical variables and the code variables, *(Recall that here $\\log$ and $\\exp$ applied to any vector shall be seen as being a shorthand for element-wise application)*\n",
        "\n",
        "\n",
        "$${\\tt (x\\_{hat}, mean, log\\_var)} :=  (\\hat{x},\\vec{\\mu_z}, 2\\cdot \\log(\\vec{\\sigma_z}))$$\n",
        "\n",
        "\n",
        "First we recall that we had defined, $p_{\\Theta}(\\vec{x} |\\vec{z}) := \\mathcal{N}({\\rm Decoder}_{\\Theta}(\\vec{z}), {\\rm diag(all{-}ones{-}vector)})$.Hence on any one input data $\\vec{x}$ the ''Reconstruction Loss'' part of the Variational Free Energy is,\n",
        "In above one is computing,\n",
        "\n",
        "> First Term of V.F.E. =  \n",
        " $$ {\\rm Reconstruction-Loss}(\\vec{x}) = -\\log p_{\\Theta}(\\vec{x}|\\vec{z}) = \\frac{1}{2}||{\\vec{x}-{\\rm Decoder}_{\\Theta}(\\vec{z})}||_2^2 = \\frac{1}{2}||\\vec{x}- \\hat{x}||_2^2 $$\n",
        "\n",
        "In above we have recalled the definition of $\\hat{x}$ from the definition of the ``forward\" function.  Further recall, that we had defined, $p(\\vec{x} |\\vec{z}) = p_{\\Theta}(\\vec{x} |\\vec{z}) := \\mathcal{N}({\\rm Decoder}_{\\Theta}(\\vec{z}), {\\rm diag(all{-}ones{-}vector)})$ -- which implies that there should have been a second term in the above ${\\rm Reconstruction-Loss}$ coming from the normalization constant in $\\mathcal{N}({\\rm Decoder}_{\\Theta}(\\vec{z}), {\\rm diag(all{-}ones{-}vector)})$ - but we shall ignore it since its a constant and that doesnt affect the gradients that would get taken w.r.t the neural weights.\n",
        "\n",
        "Now let's recall the KL formula we had seen earlier,\n",
        "\n",
        "$${\\rm KL}({\\cal N}(\\vec{\\mu},\\Sigma) || {\\cal N}(0,I) ) =  \\frac{-1}{2} \\cdot \\left [k + \\log {\\det (\\Sigma)} - \\Vert{\\vec{\\mu}}\\Vert^2 - {\\rm Tr} (\\Sigma) \\right ] $$\n",
        "\n",
        "This we need to invoke  for,\n",
        "\n",
        "$$ {\\rm KL} (q_{\\Phi}(\\vec{z} |\\vec{x}) \\Vert p(\\vec{z}) ) = {\\rm KL} ({\\cal N}(f({\\rm Encoder}_{\\Phi} (\\vec{x}))) \\Vert {\\cal N}(0,I) )$$\n",
        "\n",
        "where, we recall that, ${\\cal N}(f({\\rm Encoder}_{\\Phi} (\\vec{x}))) = {\\cal N}(\\vec{\\mu_z},\\Sigma)$ with  $\\Sigma = {\\rm diag}(\\vec{\\sigma_z} \\circ \\vec{\\sigma_z}) (= {\\rm diag} (e^{{\\tt log\\_var}} ))$ and thus we have,\n",
        "\n",
        "> Second Term of V.F.E. =\n",
        "$${\\rm KL} (q_{\\Phi}(\\vec{z} |\\vec{x}) \\Vert p(\\vec{z}) ) = -\\frac{1}{2} \\cdot \\sum_{i=1}^{\\rm latent-dimension} \\left [  1 +  ({\\rm log\\_var}_i) - {\\rm mean}_i^2 - e^{ {\\tt log\\_var_i}}   \\right ]$$\n",
        "\n",
        "\n",
        "Thus the loss being computed at data $\\vec{x}$ is,\n",
        "\n",
        "\\begin{align}\n",
        "&-\\log p_{\\Theta}(\\vec{x}|\\vec{z})  + {\\rm KL} (q_{\\Phi}(\\vec{z} |\\vec{x}) \\Vert p(\\vec{z}) )\\\\\n",
        "&= \\frac{1}{2}||\\vec{x}- \\hat{x}||_2^2 -\\frac{1}{2} \\cdot \\sum_{i=1}^{\\rm latent-dimension} \\left [  1 +  ({\\rm log\\_var}_i) - {\\rm mean}_i^2 - e^{ {\\tt log\\_var_i}}   \\right ]\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7ePacjOEsWv"
      },
      "source": [
        "# Training The VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOe9Z7ZrJGD6"
      },
      "source": [
        "Takes 12 minutes without GPU on standard (F)MNIST at the following (most tested) settings :\n",
        "\n",
        "batch_size = 100,\n",
        "hidden_dim = 400,\n",
        "latent_dim = 200,\n",
        "lr = 1e-3,\n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3aJJE1DE0z_",
        "outputId": "57e3c832-633f-48f2-cd56-ed8216a88450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The VAE training starts.\n",
            "\tEpoch 1 complete!\n",
            "\tEpoch 2 complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"The VAE training starts.\")\n",
        "model.train()\n",
        "Training_Loss = []\n",
        "Risk = []\n",
        "Epoch = []\n",
        "Risk_Epoch = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # At the beginning of each epoch we calculate the training and the test loss.\n",
        "    training_loss = 0\n",
        "    for batch_number, (r, _) in enumerate(train_loader):\n",
        "            r = r.view(batch_size, x_dim)\n",
        "            r = r.to(DEVICE)\n",
        "            r_hat, mean, log_var = model(r)\n",
        "            mini_batch_loss = loss_function(r, r_hat, mean, log_var)\n",
        "            training_loss += mini_batch_loss.item()\n",
        "            #mini_batch_loss.item() = training loss on the current mini-batch\n",
        "            #training_loss is accumulating the mini-batch losses to compute the loss on the entire training data.\n",
        "\n",
        "    Training_Loss.append(training_loss/(train_size))\n",
        "    #Now we compute the test loss at the same model parameters at which the above training loss was calculated.\n",
        "    if  epoch%5 == 0:\n",
        "        test_loss = 0\n",
        "        for test_batch_number, (t, _) in enumerate(test_loader):\n",
        "                t = t.view(test_batch_size, x_dim)\n",
        "                t = t.to(DEVICE)\n",
        "                t_hat, mean, log_var = model(t)\n",
        "                mini_batch_loss = loss_function(t, t_hat, mean, log_var)\n",
        "                test_loss += mini_batch_loss.item()\n",
        "\n",
        "        Risk.append(test_loss/(test_size))\n",
        "        Risk_Epoch.append(epoch+1)\n",
        "\n",
        "    for batch_number, (x, _) in enumerate(train_loader):\n",
        "        x = x.view(batch_size, x_dim)\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        #(x,_) pulls out a mini-batch from the train_loader which has now been converted into an enumeratable data type\n",
        "        #There is some ancilliary information attached to each mini-batch which we dont care about and that is held in that \"_\"\n",
        "        #batch_number adds 1 to itself everytime a mini-batch is pulled out\n",
        "        #Thus the batch_number counts the number of mini-batches in the training data - a number we did not know till now.\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_hat, mean, log_var = model(x)\n",
        "        mini_batch_loss = loss_function(x, x_hat, mean, log_var)\n",
        "        mini_batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"\\tEpoch\", epoch + 1, \"complete!\")\n",
        "\n",
        "    Epoch.append(epoch+1)\n",
        "\n",
        "\n",
        "print(\"The VAE training is over!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJUKX6p8UASC"
      },
      "source": [
        "## Understanding the Dynamics of the Training Loss & the Population Risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSLnZO7WTO5o"
      },
      "outputs": [],
      "source": [
        "train, = plt.plot(Epoch, Training_Loss, label=(\"Empirical Loss\"))\n",
        "test, = plt.plot(Risk_Epoch, Risk, label= (\"Population Risk\"))\n",
        "plt.legend(handles=[train, test])\n",
        "\n",
        "plt.xlabel('epoch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HqHUpzpMFOY"
      },
      "outputs": [],
      "source": [
        "print (\"Number of Training Batches =\",batch_number+1,\" & Number of Test Batches =\",test_batch_number+1)\n",
        "print (\"Mini-Batch Size =\",batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx8GaQQeepwt"
      },
      "source": [
        "# The Trained Model Can Map The Test Data to Itself - Almost!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoKycmzHeu7z"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (x, _) in enumerate(tqdm(test_loader)):\n",
        "        x = x.view(test_batch_size, x_dim)\n",
        "        x = x.to(DEVICE)\n",
        "        x_hat, _, _ = model(x)\n",
        "        break\n",
        "\n",
        "\n",
        "def show_image(x, idx):\n",
        "    x = x.view(test_batch_size, 28, 28)\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(x[idx].cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbY-o3Bd6R1C"
      },
      "source": [
        "Recall that the later of the 2 images ${\\tt x\\_hat}$($\\tt idx$ element of it) is the output when the VAE gets the fist image ${\\tt x}$($\\tt idx$ element of it) as input.\n",
        "\n",
        "Thus the two images being close implies that the VAE was able to almost successfully reconstruct its input - as it was needed to do!\n",
        "\n",
        "Also note that the above test was run on test data - thats unseen data for the training - and hence this is testing **``generalizability\"**\n",
        "\n",
        "--  i.e whether the VAE is doing the good reconstructions on new data that it was not trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEXNA1x0AhY-"
      },
      "outputs": [],
      "source": [
        "def train_model(model, epochs, train_loader, test_loader):\n",
        "    model.train()\n",
        "    Test_Risk = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "\n",
        "        for batch_number, (x, _) in enumerate(train_loader):\n",
        "            x = x.view(batch_size, x_dim)\n",
        "            x = x.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            x_hat, mean, log_var = model(x)\n",
        "            mini_batch_loss = loss_function(x, x_hat, mean, log_var)\n",
        "            mini_batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        test_loss = 0\n",
        "    for test_batch_number, (t, _) in enumerate(test_loader):\n",
        "            t = t.view(test_batch_size, x_dim)\n",
        "            t = t.to(DEVICE)\n",
        "            t_hat, mean, log_var = model(t)\n",
        "            mini_batch_loss = loss_function(t, t_hat, mean, log_var)\n",
        "            test_loss += mini_batch_loss.item()\n",
        "\n",
        "    Test_Risk.append(test_loss/(test_size))\n",
        "\n",
        "    return {\n",
        "    \"Test_Risk\":             Test_Risk,\n",
        "    }\n",
        "\n",
        "# Model parameters (here for posterity)\n",
        "x_dim  = 784\n",
        "hidden_dim = 1024\n",
        "latent_dim = 8\n",
        "input_dim = output_dim = x_dim\n",
        "\n",
        "# Training parameters\n",
        "lr = 1e-3\n",
        "train_size = 60000\n",
        "batch_size = 10**2\n",
        "test_batch_size = 10**4\n",
        "epochs = 10\n",
        "\n",
        "# Variable Parameters\n",
        "from itertools import product\n",
        "hidden_dims = 2 ** np.arange(7, 11)\n",
        "latent_dims = 2 ** np.arange(0, 8)\n",
        "param_iterable = product(hidden_dims, latent_dims)\n",
        "\n",
        "# Cache\n",
        "data_cache = {}\n",
        "\n",
        "for i, (hidden_dim, latent_dim) in enumerate(param_iterable):\n",
        "    encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
        "    decoder = Decoder(latent_dim, hidden_dim, output_dim)\n",
        "    model = Model(encoder, decoder).to(DEVICE)\n",
        "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
        "    results = train_model(model, epochs, train_loader, test_loader)\n",
        "\n",
        "    data_cache[f'{hidden_dim}|{latent_dim}'] = results\n",
        "    data_cache[f'{hidden_dim}|{latent_dim}']['total_parameters'] = \\\n",
        "        parameter_count(encoder)[1] + parameter_count(decoder)[1]\n",
        "    print(f'Model {hidden_dim}, {latent_dim} Risk:',results['Test_Risk'][-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_cache"
      ],
      "metadata": {
        "id": "jgvc2JpKMDJT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}